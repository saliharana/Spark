{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFRGQECDYrU6",
        "outputId": "b883d862-fa5d-4ecb-c1f6-78dc5a58ff19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=c179a76193135f4e000dfc89d259495cc440674bbe62277b3b550d04d7f4c978\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ovrkjojnY_F9"
      },
      "outputs": [],
      "source": [
        "#spark kodu database e gotrup calistirdigi icin diger veri tabanlarina gore daha hizli calisiyor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WX5lk0YoZ8IX"
      },
      "outputs": [],
      "source": [
        "#spark verileri tablo olarak degil satir olarak okudugu icin satirlari birlestiriyoruz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xd1RqokbCuY",
        "outputId": "c186c43b-93bc-4717-a06b-71273789f15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16, 25, 4, 4]\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext #conf ve icerik dosyalari\n",
        "import collections#toplamak icin\n",
        "\n",
        "sc = SparkContext()\n",
        "rdd = sc.parallelize([4, 5, 2, 2])#rdd sparkin degisken adi  paralel hesaplama\n",
        "\n",
        "sq = rdd.map(lambda x: x*x) #yukardaki verilerin karesini aldik\n",
        "\n",
        "print(sq.collect())#hesap sonuclarini topluyor\n",
        "\n",
        "sc.stop()#1 kere calistir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULQhEnIf1iSH"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veri setini ice aktariyoruz"
      ],
      "metadata": {
        "id": "FuPWlzR7oYYK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"veriokuma\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"churn.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "wOmKg7krkSdw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA"
      ],
      "metadata": {
        "id": "c22S4-NyogB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIeor8lykaFQ",
        "outputId": "dacf681a-e234-4244-cff2-1636139b73f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
            "|_c0|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|\n",
            "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
            "|  0|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|    1|\n",
            "|  1|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|    1|\n",
            "|  2|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|    1|\n",
            "|  3|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|    1|\n",
            "|  4|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|    1|\n",
            "|  5|Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|    1|\n",
            "|  6|     Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|    1|\n",
            "|  7|   Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|    1|\n",
            "|  8|     Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|    1|\n",
            "|  9|  Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|    1|\n",
            "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = df.groupBy(\"Churn\").count()\n",
        "value_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw38nNXolFN-",
        "outputId": "2a35c7b6-a6c2-4d2d-8f95-c94aed1e53f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|Churn|count|\n",
            "+-----+-----+\n",
            "|    1|  150|\n",
            "|    0|  750|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atJia_vAlMFG",
        "outputId": "b3763dfa-79c0-4d9c-e2d6-227ac3974c95"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnKawSKjlMJG",
        "outputId": "e2242243-617f-4321-8c94-66e2274d2e7b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
            "|summary|               _c0|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|              Churn|\n",
            "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
            "|  count|               900|          900|              900|              900|               900|              900|               900|                900|\n",
            "|   mean|             449.5|         NULL|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|0.16666666666666666|\n",
            "| stddev|259.95191863111916|         NULL|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969| 0.3728852122772358|\n",
            "|    min|                 0|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|                  0|\n",
            "|    max|               899|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|                  1|\n",
            "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "null_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7DdaWf6lMMM",
        "outputId": "c5dda52e-b062-46e3-b04d-26b2527507a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+--------------+---------------+-----+---------+-----+\n",
            "|_c0|Names|Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|\n",
            "+---+-----+---+--------------+---------------+-----+---------+-----+\n",
            "|  0|    0|  0|             0|              0|    0|        0|    0|\n",
            "+---+-----+---+--------------+---------------+-----+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODELLING"
      ],
      "metadata": {
        "id": "SZb_3A40oktk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "# Sayısal özellik sütunlarını seçin\n",
        "numeric_cols = ['Age','Total_Purchase','Account_Manager','Years','Num_Sites']\n",
        "\n",
        "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
        "\n",
        "# Veri kümesini özellik vektörleri ve hedef sütunu olarak hazırlayın\n",
        "data = assembler.transform(df).select(\"features\", \"Churn\")\n",
        "\n",
        "# Eğitim ve test verisi olarak bölün\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# GBTClassifier modelini oluşturun\n",
        "gbt = GBTClassifier(labelCol=\"Churn\", featuresCol=\"features\")\n",
        "\n",
        "# Pipeline oluşturun\n",
        "pipeline = Pipeline(stages=[gbt])\n",
        "\n",
        "# Modeli eğitin\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Test verileri üzerinde tahmin yapın\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Tahminleri değerlendirin\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(\"AUC:\", auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7_TvKsfledo",
        "outputId": "88058138-5b8a-4245-fcb9-9f83ae1f072a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.8431148373983745\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}